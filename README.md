# Neural Network from Scratch with Optimization
@author: Soumya

This project implements a fully connected feedforward neural network from scratch using Python and NumPy. It includes key components such as forward propagation, backpropagation, and optimization techniques like gradient descent. The implementation is done in a Jupyter Notebook for clarity and experimentation.

## 🚀 Features

- **Manual Implementation** – No high-level ML libraries like TensorFlow or PyTorch used.
- **Forward and Backward Propagation** – Core neural network algorithms implemented from scratch.
- **Gradient Descent Optimization** – Updates weights via calculated gradients.
- **Interactive Jupyter Notebook** – Easy to read, run, and tweak for experiments.

## 📁 Project Structure

```
Neural-Network/
├── Neural_Network.ipynb    # Main Jupyter Notebook with the neural network implementation
```

## 🧠 Concepts Covered

- Weight & Bias Initialization
- Activation Functions
- Loss Function (Mean Squared Error)
- Forward Propagation
- Backpropagation (Gradient Calculation)
- Weight Updates using Gradient Descent

## ✅ Requirements

- Python 3.x
- NumPy
- Jupyter Notebook

Install dependencies with:

```bash
pip install numpy notebook
```

## ▶️ Running the Notebook

1. Clone the repository:

```bash
git clone https://github.com/riCl3/Neural-Network.git
cd Neural-Network
```

2. Checkout the `optimization` branch:

```bash
git checkout optimization
```

3. Launch the notebook:

```bash
jupyter notebook
```

4. Open `Neural_Network.ipynb` and run the cells.

## 💪 Customization

You can easily change:
- The number of input, hidden, and output nodes
- Activation functions
- Learning rate
- Dataset used

Feel free to experiment and observe how it affects the model’s learning.

## 🤝 Contributing

Pull requests and suggestions are welcome! If you find a bug or want to improve the implementation, feel free to contribute.

## 📜 License

This project is licensed under the [MIT License](LICENSE).
